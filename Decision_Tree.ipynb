{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n",
        "## Assignment Questions"
      ],
      "metadata": {
        "id": "1tpyT4ml5I8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. In classification, it splits the dataset into smaller subsets based on feature values, forming a tree-like structure with decision nodes and leaf nodes. Each internal node represents a feature test, each branch represents an outcome, and each leaf node represents a class label. The model works by recursively choosing the feature that best separates the data according to a certain criterion (e.g., Gini Impurity or Entropy) until it reaches pure leaf nodes or satisfies stopping conditions.\n",
        "\n",
        "\n",
        "### 2 Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:\n",
        "Gini Impurity: Measures the probability of incorrectly classifying a randomly chosen element if it was labeled according to the distribution of labels in the node. Formula:\n",
        "\n",
        "Gini=1 - i=1 ∑n pi2\n",
        "is the proportion of class\n",
        "i in the node. Lower Gini means higher purity.\n",
        "Entropy: Measures the amount of uncertainty or disorder in the data. Formula:\n",
        "Entropy=- is=1 ∑ n pi log2(pi)\n",
        "Lower entropy means higher purity.\n",
        "Impact on splits:\n",
        "Both measures are used to decide the best split. The feature and threshold that lead to the greatest reduction in impurity (highest Information Gain for entropy or highest Gini reduction) are chosen for splitting.\n",
        "\n",
        "\n",
        "### 3 What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning: Stops the growth of the tree early based on certain conditions (e.g., max depth, min samples split).\n",
        "Advantage: Prevents overfitting and reduces training time.\n",
        "Post-Pruning: Allows the tree to grow fully and then removes branches that do not contribute significantly to accuracy, based on validation data.\n",
        "Advantage: Produces a simpler model while maintaining accuracy.\n",
        "\n",
        "\n",
        "### 4 What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "Information Gain measures the reduction in entropy (uncertainty) after splitting a dataset based on a feature. It is calculated as:\n",
        "IG=Entropy parent ​ − i ∑ ​ ∣D∣ ∣D i ​ ∣ ​ ×Entropy(D i ​ )\n",
        "A higher Information Gain means the split produces purer child nodes. It is important because it helps select the feature that provides the most useful separation of the classes at each step of building the tree.\n",
        "\n",
        "\n",
        "### 5 What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "Applications:\n",
        "Medical diagnosis (predicting diseases)\n",
        "Credit scoring and loan approval\n",
        "Customer segmentation in marketing\n",
        "Predicting equipment failure in manufacturing\n",
        "Advantages:\n",
        "Easy to understand and interpret\n",
        "Handles both numerical and categorical data\n",
        "No need for extensive data preprocessing\n",
        "Limitations:\n",
        "Prone to overfitting on training data\n",
        "Can be unstable with small data changes\n",
        "Biased towards features with more levels"
      ],
      "metadata": {
        "id": "IN4Ao6pA8OoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Feature Importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYD9RFscE9v5",
        "outputId": "733d1a0c-013c-4c6d-c138-404cc651ed04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "2  petal length (cm)    0.906143\n",
            "3   petal width (cm)    0.077186\n",
            "1   sepal width (cm)    0.016670\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fully grown tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Tree with max_depth = 3\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_acc = accuracy_score(y_test, limited_tree.predict(X_test))\n",
        "\n",
        "print(f\"Fully-grown tree accuracy: {full_acc:.2f}\")\n",
        "print(f\"Max depth=3 tree accuracy: {limited_acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOYBnd4TFfqL",
        "outputId": "53916cc0-3c55-41d9-af1d-591dcf0f755f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.00\n",
            "Max depth=3 tree accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions & MSE\n",
        "y_pred = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': housing.feature_names,\n",
        "    'Importance': reg.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RhP_RCxHKlz",
        "outputId": "b46e5606-50b4-494e-9fe5-fd1f5250f754"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.50\n",
            "\n",
            "Feature Importances:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.528509\n",
            "5    AveOccup    0.130838\n",
            "6    Latitude    0.093717\n",
            "7   Longitude    0.082902\n",
            "2    AveRooms    0.052975\n",
            "1    HouseAge    0.051884\n",
            "4  Population    0.030516\n",
            "3   AveBedrms    0.028660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters & accuracy\n",
        "best_params = grid.best_params_\n",
        "best_score = grid.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Cross-validated Accuracy: {best_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQgoeXZmGrfT",
        "outputId": "37ec87d8-15cd-4e08-a59d-a9d720259a77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Cross-validated Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)\n",
        "\n",
        "Answer:\n",
        "\n",
        "Step-by-step process:\n",
        "1)Handle the missing values:\n",
        "For numerical features: Replace missing values with the mean or median.\n",
        "For categorical features: Replace missing values with the mode or create a separate category like \"Unknown\".\n",
        "\n",
        "2)Encode the categorical features:\n",
        "Use One-Hot Encoding for nominal categories (e.g., gender, blood type).\n",
        "Use Label Encoding or Ordinal Encoding if the categories have an inherent order (e.g., disease severity levels).\n",
        "\n",
        "3)Train a Decision Tree model:\n",
        "Split the dataset into training and testing sets (e.g., 80% train, 20% test).\n",
        "Initialize and train a DecisionTreeClassifier on the training data.\n",
        "\n",
        "4)Tune its hyperparameters:\n",
        "Adjust parameters like max_depth, min_samples_split, and criterion (Gini or Entropy) using GridSearchCV or RandomizedSearchCV to improve performance.\n",
        "\n",
        "5)Evaluate its performance:\n",
        "Use metrics such as accuracy, precision, recall, F1-score, and confusion matrix to assess prediction quality.\n",
        "Perform cross-validation to ensure model robustness.\n",
        "\n",
        "6)Business value in a real-world setting:\n",
        "Early prediction of diseases allows timely intervention, reducing treatment costs and improving patient outcomes.\n",
        "Helps healthcare providers prioritize high-risk patients for preventive care.\n",
        "Supports data-driven decisions in hospital resource allocation and personalized treatment plans.\n",
        "\n"
      ],
      "metadata": {
        "id": "wNmCtAOSH8dQ"
      }
    }
  ]
}